# ===================== Parameters for Supervised Mode =====================
# Training parameters
epochs: 100               # Number of global training epochs
lr: 1e-4                  # Learning rate (1e-4 is the initial hyperparameter)
batch_size: 64            # Batch size
weight_decay: 1e-2        # Regularization strength

# Learning rate scheduler parameters
lr_scheduler: StepLR      # Options: 'StepLR', 'ReduceLROnPlateau', 'CosineAnnealingLR'
lr_step_size: 5           # Step size for StepLR
lr_gamma: 0.8             # Learning rate decay factor (0.8, 0.5 may be unstable)

lr_min: 1e-7              # Minimum learning rate
lr_patience: 5            # Patience for ReduceLROnPlateau (5 is better for supervised)
lr_T_max: 60              # T_max for CosineAnnealingLR (30 works well, 30-100; for supervised, should match epochs)

# Logging
recordfile: log/log.txt
resultfile: results/fedavg_results.xlsx

# Dataset
train_image_root: the/route/to/your/dataset
test_image_root: the/route/to/your/dataset
metadata_path: the/route/to/your/metadata.csv
ham10000_image_root: the/route/to/your/HAM10000/images
# ===================== Parameters for Supervised Mode =====================

# ===================== Multi-site Parameters =======================
num_clients: 5                # Number of clients
fedavg_num: 10                # Number of epochs per averaging
lambda_prox: 0.1              # FedProx
consistency_lambda: 0.01      # SFMed consistency loss ratio
avg_interval: 5               # Interval for server model averaging
# ===================== fedavg Parameters =======================